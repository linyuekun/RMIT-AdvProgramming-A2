{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 2&3\n",
    "#### Student Name: Sukhum Boondecharak\n",
    "#### Student ID: S3940976\n",
    "\n",
    "Date: 04 Oct 2023\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include all the libraries you used in your assignment, e.g.,:\n",
    "* pandas\n",
    "* re\n",
    "* numpy\n",
    "\n",
    "## Introduction\n",
    "The objective for task 2 is to create feature representations for job advertisement descriptions. These representations will be used to capture the essential information within the text data, making it suitable for machine learning models. The task involves two main feature generation processes:\n",
    "\n",
    "1. Bag-of-Words Model: This approach involves creating count vector representations for each job advertisement description based on a preprocessed vocabulary. The generated count vectors represent the frequency of each word in the descriptions.\n",
    "2. Word Embeddings: This is to capture semantic relationships between words and can provide rich representations for text data. In this sub-task, I chose FastText as a word embedding model and initially created both unweighted and TF-IDF weighted vector representations for job advertisement descriptions.\n",
    "\n",
    "Task 3 focuses on building machine learning models to classify job advertisements into specific categories based on their textual content. The primary goal is to investigate two key questions:\n",
    "\n",
    "- Q1: Which language model, among those created in Task 2, performs best when combined with chosen machine learning models? Various models will be built based on different feature representations, and their performance will be evaluated.\n",
    "\n",
    "- Q2: Does more information improve accuracy? Different combinations of features will be explored, including using only the job title, only the job description, or both. By experimenting with these combinations, We aim to understand whether incorporating additional information, such as job titles, improves the accuracy of the classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries \n",
    "\n",
    "Various libraries are imported for different activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from gensim.models.fasttext import FastText\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import load_files  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from itertools import chain\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Generating Feature Representations for Job Advertisement Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words (BoW) Model: Count Vectors\n",
    "\n",
    "First, generate the Count vector representation for each job advertisement description using the vocabulary created in Task 1. The count vectors will be combined and saved at the end of this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['72444142',\n",
       " '68687567',\n",
       " '68257980',\n",
       " '71168766',\n",
       " '72441930',\n",
       " '70205492',\n",
       " '69929266',\n",
       " '68814305',\n",
       " '71737507',\n",
       " '69540434',\n",
       " '71199751',\n",
       " '70457475',\n",
       " '72411451',\n",
       " '69001764',\n",
       " '71171544',\n",
       " '68057786',\n",
       " '69040220',\n",
       " '68784018',\n",
       " '69146761',\n",
       " '68256016',\n",
       " '70251801',\n",
       " '68180459',\n",
       " '68056671',\n",
       " '72448172',\n",
       " '69250788',\n",
       " '67639091',\n",
       " '68256188',\n",
       " '69577650',\n",
       " '66600427',\n",
       " '71339723',\n",
       " '72691163',\n",
       " '69989027',\n",
       " '69635720',\n",
       " '69993409',\n",
       " '68356152',\n",
       " '72439398',\n",
       " '69577820',\n",
       " '68708197',\n",
       " '67895483',\n",
       " '71848552',\n",
       " '68257221',\n",
       " '68686791',\n",
       " '68806418',\n",
       " '69188332',\n",
       " '68062805',\n",
       " '68678164',\n",
       " '72680220',\n",
       " '69553242',\n",
       " '71633491',\n",
       " '71171000',\n",
       " '71677705',\n",
       " '62269820',\n",
       " '71677311',\n",
       " '70597736',\n",
       " '68258658',\n",
       " '68553492',\n",
       " '71596865',\n",
       " '69694967',\n",
       " '71750603',\n",
       " '72457901',\n",
       " '70597879',\n",
       " '72673887',\n",
       " '70599432',\n",
       " '71198896',\n",
       " '72672874',\n",
       " '71678606',\n",
       " '69250648',\n",
       " '71848359',\n",
       " '72233918',\n",
       " '70190910',\n",
       " '70256074',\n",
       " '68062611',\n",
       " '66887344',\n",
       " '72240625',\n",
       " '68257449',\n",
       " '70520065',\n",
       " '71849489',\n",
       " '69671186',\n",
       " '69799351',\n",
       " '69000079',\n",
       " '71170782',\n",
       " '68688072',\n",
       " '55414504',\n",
       " '67332022',\n",
       " '71184796',\n",
       " '72397595',\n",
       " '72578233',\n",
       " '72232029',\n",
       " '71745516',\n",
       " '69770990',\n",
       " '67970848',\n",
       " '69056135',\n",
       " '68095319',\n",
       " '72173738',\n",
       " '65451652',\n",
       " '71082952',\n",
       " '72236534',\n",
       " '72241506',\n",
       " '71185283',\n",
       " '68688500',\n",
       " '68194862',\n",
       " '71288404',\n",
       " '72460729',\n",
       " '72446515',\n",
       " '71240896',\n",
       " '72241841',\n",
       " '67639020',\n",
       " '71083870',\n",
       " '69687048',\n",
       " '68347634',\n",
       " '72438284',\n",
       " '68182091',\n",
       " '72446522',\n",
       " '69799803',\n",
       " '68608728',\n",
       " '69147324',\n",
       " '72234499',\n",
       " '68697153',\n",
       " '68784692',\n",
       " '71340157',\n",
       " '68807391',\n",
       " '68057065',\n",
       " '69556224',\n",
       " '68697352',\n",
       " '66753565',\n",
       " '69086048',\n",
       " '68180326',\n",
       " '70028246',\n",
       " '70485873',\n",
       " '55415689',\n",
       " '72448496',\n",
       " '69600219',\n",
       " '69188766',\n",
       " '69000974',\n",
       " '68346273',\n",
       " '69598361',\n",
       " '69004747',\n",
       " '72401303',\n",
       " '62004449',\n",
       " '71800229',\n",
       " '65101527',\n",
       " '68780456',\n",
       " '72227008',\n",
       " '71847011',\n",
       " '69170925',\n",
       " '70028343',\n",
       " '69250765',\n",
       " '68678482',\n",
       " '68576082',\n",
       " '68258357',\n",
       " '71679411',\n",
       " '66399629',\n",
       " '68702817',\n",
       " '66935937',\n",
       " '71812011',\n",
       " '69768475',\n",
       " '69092773',\n",
       " '71674555',\n",
       " '70758175',\n",
       " '69564390',\n",
       " '70255258',\n",
       " '68510409',\n",
       " '68675667',\n",
       " '72238746',\n",
       " '68711980',\n",
       " '68677803',\n",
       " '71852020',\n",
       " '71393735',\n",
       " '69072950',\n",
       " '71555733',\n",
       " '72421706',\n",
       " '72239537',\n",
       " '72236150',\n",
       " '65450489',\n",
       " '72661895',\n",
       " '69805538',\n",
       " '69594171',\n",
       " '72451165',\n",
       " '71671543',\n",
       " '70757932',\n",
       " '50738105',\n",
       " '68677615',\n",
       " '70758173',\n",
       " '68681216',\n",
       " '69024685',\n",
       " '68292692',\n",
       " '68678658',\n",
       " '69073451',\n",
       " '71361732',\n",
       " '69046353',\n",
       " '71748554',\n",
       " '66887701',\n",
       " '70758181',\n",
       " '68682048',\n",
       " '68693284',\n",
       " '70593580',\n",
       " '69931225',\n",
       " '72460315',\n",
       " '72198878',\n",
       " '72479442',\n",
       " '71677246',\n",
       " '71838397',\n",
       " '62017964',\n",
       " '68714905',\n",
       " '71293696',\n",
       " '71393784',\n",
       " '72421504',\n",
       " '69858761',\n",
       " '72457492',\n",
       " '68669224',\n",
       " '68237569',\n",
       " '68691974',\n",
       " '70163439',\n",
       " '71808636',\n",
       " '69170518',\n",
       " '68177629',\n",
       " '70656711',\n",
       " '68675231',\n",
       " '69956980',\n",
       " '66749246',\n",
       " '70253925',\n",
       " '71851935',\n",
       " '62011444',\n",
       " '68564061',\n",
       " '72244156',\n",
       " '68715062',\n",
       " '72457200',\n",
       " '71430843',\n",
       " '71189966',\n",
       " '71293725',\n",
       " '69802421',\n",
       " '70223375',\n",
       " '71686822',\n",
       " '71188781',\n",
       " '71857143',\n",
       " '71291615',\n",
       " '71851737',\n",
       " '68699701',\n",
       " '67896489',\n",
       " '71293654',\n",
       " '69856091',\n",
       " '69065735',\n",
       " '69685775',\n",
       " '62016897',\n",
       " '67098815',\n",
       " '70676738',\n",
       " '69544219',\n",
       " '68063266',\n",
       " '68695864',\n",
       " '64587110',\n",
       " '68676602',\n",
       " '71561249',\n",
       " '69930907',\n",
       " '68994913',\n",
       " '64602169',\n",
       " '69087918',\n",
       " '66643191',\n",
       " '72661545',\n",
       " '71443726',\n",
       " '69792349',\n",
       " '72479164',\n",
       " '70757997',\n",
       " '69172926',\n",
       " '68242247',\n",
       " '72200044',\n",
       " '69801399',\n",
       " '68714140',\n",
       " '67996688',\n",
       " '67098684',\n",
       " '70436315',\n",
       " '67802489',\n",
       " '72242359',\n",
       " '69962730',\n",
       " '72384220',\n",
       " '68678435',\n",
       " '68683343',\n",
       " '69169496',\n",
       " '72555421',\n",
       " '71240775',\n",
       " '71432341',\n",
       " '68214614',\n",
       " '72460233',\n",
       " '72338560',\n",
       " '67018488',\n",
       " '71708131',\n",
       " '68700672',\n",
       " '71432102',\n",
       " '70253135',\n",
       " '62004211',\n",
       " '68708318',\n",
       " '71903625',\n",
       " '69088253',\n",
       " '67949521',\n",
       " '69688299',\n",
       " '71142126',\n",
       " '69600287',\n",
       " '70159396',\n",
       " '68612091',\n",
       " '70757636',\n",
       " '68622279',\n",
       " '70420139',\n",
       " '71621313',\n",
       " '67099419',\n",
       " '68299652',\n",
       " '68408186',\n",
       " '62017143',\n",
       " '71847666',\n",
       " '68709520',\n",
       " '69228108',\n",
       " '71375118',\n",
       " '68995641',\n",
       " '68802053',\n",
       " '72115771',\n",
       " '72450640',\n",
       " '67931789',\n",
       " '69931161',\n",
       " '69545040',\n",
       " '72447151',\n",
       " '72481557',\n",
       " '71674616',\n",
       " '68608928',\n",
       " '68538230',\n",
       " '69030000',\n",
       " '69073629',\n",
       " '72338070',\n",
       " '68997528',\n",
       " '68691589',\n",
       " '69746362',\n",
       " '71635418',\n",
       " '71428404',\n",
       " '69673592',\n",
       " '68063513',\n",
       " '71186780',\n",
       " '71711622',\n",
       " '71356489',\n",
       " '70656648',\n",
       " '68242598',\n",
       " '69803404',\n",
       " '69937154',\n",
       " '67174811',\n",
       " '69063925',\n",
       " '68684698',\n",
       " '69035657',\n",
       " '68806037',\n",
       " '71139623',\n",
       " '68702096',\n",
       " '68531828',\n",
       " '48844742',\n",
       " '72342158',\n",
       " '71402732',\n",
       " '66426163',\n",
       " '70599043',\n",
       " '66747476',\n",
       " '69568536',\n",
       " '68573837',\n",
       " '70598762',\n",
       " '72441328',\n",
       " '69192492',\n",
       " '69580237',\n",
       " '70016153',\n",
       " '53863788',\n",
       " '68507612',\n",
       " '68508976',\n",
       " '68310427',\n",
       " '72609298',\n",
       " '72609591',\n",
       " '58936930',\n",
       " '69182884',\n",
       " '72627933',\n",
       " '68537806',\n",
       " '70013065',\n",
       " '46629905',\n",
       " '71691899',\n",
       " '70682023',\n",
       " '68560763',\n",
       " '68718213',\n",
       " '68728610',\n",
       " '70016465',\n",
       " '46635135',\n",
       " '71675717',\n",
       " '46627840',\n",
       " '69539327',\n",
       " '69010815',\n",
       " '72226412',\n",
       " '70090631',\n",
       " '54929044',\n",
       " '71295925',\n",
       " '71609787',\n",
       " '46633806',\n",
       " '72339917',\n",
       " '69817467',\n",
       " '70253936',\n",
       " '70763720',\n",
       " '69220730',\n",
       " '70623750',\n",
       " '68564494',\n",
       " '46626933',\n",
       " '70598851',\n",
       " '68559978',\n",
       " '66578075',\n",
       " '69783619',\n",
       " '70763753',\n",
       " '70148800',\n",
       " '69973695',\n",
       " '68510120',\n",
       " '68302335',\n",
       " '68537749',\n",
       " '67959741',\n",
       " '62007842',\n",
       " '68719557',\n",
       " '68560477',\n",
       " '71556854',\n",
       " '69011871',\n",
       " '71335031',\n",
       " '71608168',\n",
       " '70756164',\n",
       " '55400426',\n",
       " '71750535',\n",
       " '70763481',\n",
       " '69196531',\n",
       " '71337749',\n",
       " '69987622',\n",
       " '71233429',\n",
       " '68718817',\n",
       " '72342085',\n",
       " '69615159',\n",
       " '72340106',\n",
       " '60685363',\n",
       " '71080674',\n",
       " '68718720',\n",
       " '62524959',\n",
       " '72702961',\n",
       " '67444373',\n",
       " '72443411',\n",
       " '70474220',\n",
       " '69996401',\n",
       " '71204423',\n",
       " '71402478',\n",
       " '67746089',\n",
       " '71082148',\n",
       " '66952266',\n",
       " '70016152',\n",
       " '68720930',\n",
       " '66544069',\n",
       " '46628059',\n",
       " '72609667',\n",
       " '71230720',\n",
       " '47920414',\n",
       " '69689688',\n",
       " '64805954',\n",
       " '70232739',\n",
       " '71802875',\n",
       " '69686571',\n",
       " '70763867',\n",
       " '59968160',\n",
       " '69199873',\n",
       " '71805092',\n",
       " '70448586',\n",
       " '68507605',\n",
       " '72184995',\n",
       " '71885903',\n",
       " '50882000',\n",
       " '66932711',\n",
       " '68310471',\n",
       " '69688172',\n",
       " '71353597',\n",
       " '51061143',\n",
       " '72184029',\n",
       " '68560905',\n",
       " '71793578',\n",
       " '68310385',\n",
       " '71841735',\n",
       " '70264328',\n",
       " '68720237',\n",
       " '71614229',\n",
       " '69012306',\n",
       " '46632140',\n",
       " '70627602',\n",
       " '69973851',\n",
       " '66601501',\n",
       " '71606900',\n",
       " '46629831',\n",
       " '71903513',\n",
       " '67151783',\n",
       " '71335886',\n",
       " '72692186',\n",
       " '69011089',\n",
       " '71692209',\n",
       " '70265318',\n",
       " '72192734',\n",
       " '68574254',\n",
       " '70014362',\n",
       " '67959409',\n",
       " '68709313',\n",
       " '71109791',\n",
       " '71805339',\n",
       " '71296110',\n",
       " '68537277',\n",
       " '69580390',\n",
       " '72481346',\n",
       " '68731887',\n",
       " '67749541',\n",
       " '71885864',\n",
       " '68061775',\n",
       " '46634833',\n",
       " '72609755',\n",
       " '71602572',\n",
       " '62011450',\n",
       " '69146692',\n",
       " '69817624',\n",
       " '71094924',\n",
       " '72241686',\n",
       " '70692809',\n",
       " '55278133',\n",
       " '71803987',\n",
       " '69568022',\n",
       " '70086531',\n",
       " '72186225',\n",
       " '46627260',\n",
       " '68700336',\n",
       " '69748492',\n",
       " '68372648',\n",
       " '71841765',\n",
       " '72287638',\n",
       " '70205656',\n",
       " '69191349',\n",
       " '56209809',\n",
       " '69768322',\n",
       " '72226795',\n",
       " '66952130',\n",
       " '69266150',\n",
       " '71803856',\n",
       " '71335000',\n",
       " '69119013',\n",
       " '69587191',\n",
       " '72118191',\n",
       " '72694003',\n",
       " '68576341',\n",
       " '71691917',\n",
       " '68728668',\n",
       " '71812575',\n",
       " '71090289',\n",
       " '46629059',\n",
       " '72478300',\n",
       " '69783468',\n",
       " '69551029',\n",
       " '68805556',\n",
       " '69265319',\n",
       " '70256561',\n",
       " '72666862',\n",
       " '66630867',\n",
       " '72453749',\n",
       " '72635560',\n",
       " '71099289',\n",
       " '71818770',\n",
       " '70076682',\n",
       " '72689197',\n",
       " '68218431',\n",
       " '70807997',\n",
       " '69198249',\n",
       " '69079778',\n",
       " '71851898',\n",
       " '71099880',\n",
       " '72485752',\n",
       " '72236089',\n",
       " '71125336',\n",
       " '71619468',\n",
       " '71679611',\n",
       " '69079190',\n",
       " '69213729',\n",
       " '70218883',\n",
       " '69145960',\n",
       " '69536987',\n",
       " '68217600',\n",
       " '69267760',\n",
       " '71631590',\n",
       " '72117596',\n",
       " '70762357',\n",
       " '72444694',\n",
       " '67442211',\n",
       " '70250035',\n",
       " '72454550',\n",
       " '70077718',\n",
       " '71816988',\n",
       " '67333526',\n",
       " '68705166',\n",
       " '69145949',\n",
       " '69083137',\n",
       " '68179675',\n",
       " '72481581',\n",
       " '72634040',\n",
       " '71850659',\n",
       " '71796980',\n",
       " '72485443',\n",
       " '69867890',\n",
       " '69081850',\n",
       " '71367580',\n",
       " '66298393',\n",
       " '67946086',\n",
       " '71125587',\n",
       " '68679476',\n",
       " '71369502',\n",
       " '71856853',\n",
       " '71125819',\n",
       " '68217095',\n",
       " '68546047',\n",
       " '69966873',\n",
       " '71196021',\n",
       " '72404995',\n",
       " '70076688',\n",
       " '69079811',\n",
       " '71369684',\n",
       " '71230837',\n",
       " '67958106',\n",
       " '72443359',\n",
       " '69079850',\n",
       " '72690861',\n",
       " '71685513',\n",
       " '70218831',\n",
       " '69052698',\n",
       " '71199807',\n",
       " '68181436',\n",
       " '64809893',\n",
       " '71215909',\n",
       " '72395107',\n",
       " '69190420',\n",
       " '70675910',\n",
       " '69501426',\n",
       " '68218446',\n",
       " '72337118',\n",
       " '72531481',\n",
       " '66905324',\n",
       " '70162275',\n",
       " '67946221',\n",
       " '67304988',\n",
       " '69079024',\n",
       " '68824328',\n",
       " '72395002',\n",
       " '72444583',\n",
       " '67949291',\n",
       " '70322392',\n",
       " '69966126',\n",
       " '69993139',\n",
       " '70755882',\n",
       " '69669856',\n",
       " '69080771',\n",
       " '70676894',\n",
       " '72238038',\n",
       " '72406398',\n",
       " '72241526',\n",
       " '69267364',\n",
       " '71469076',\n",
       " '66506073',\n",
       " '70607796',\n",
       " '71443055',\n",
       " '67946161',\n",
       " '69171545',\n",
       " '69081099',\n",
       " '72443521',\n",
       " '69848827',\n",
       " '69082147',\n",
       " '68218566',\n",
       " '64144831',\n",
       " '64216173',\n",
       " '70608442',\n",
       " '72427959',\n",
       " '68544610',\n",
       " '70309482',\n",
       " '69551242',\n",
       " '68700091',\n",
       " '72479966',\n",
       " '71186393',\n",
       " '71812293',\n",
       " '69557422',\n",
       " '72405497',\n",
       " '69805343',\n",
       " '62113808',\n",
       " '71224914',\n",
       " '71443077',\n",
       " '68057472',\n",
       " '70677481',\n",
       " '72564385',\n",
       " '72630830',\n",
       " '68704193',\n",
       " '70321511',\n",
       " '69147753',\n",
       " '70077260',\n",
       " '69671196',\n",
       " '72691194',\n",
       " '68707581',\n",
       " '67753890',\n",
       " '72662930',\n",
       " '69805267',\n",
       " '71314005',\n",
       " '68346097',\n",
       " '71821372',\n",
       " '70207759',\n",
       " '69083316',\n",
       " '72351069',\n",
       " '71851119',\n",
       " '72628995',\n",
       " '66981532',\n",
       " '72441022',\n",
       " '72442746',\n",
       " '70218857',\n",
       " '72447529',\n",
       " '72690823',\n",
       " '72690576',\n",
       " '72198584',\n",
       " '71763997',\n",
       " '69080365',\n",
       " '69600129',\n",
       " '69264935',\n",
       " '69084078',\n",
       " '71470776',\n",
       " '72444600',\n",
       " '68712859',\n",
       " '72480999',\n",
       " '66673865',\n",
       " '68548006',\n",
       " '72679817',\n",
       " '68591444',\n",
       " '72351795',\n",
       " '69168938',\n",
       " '72442494',\n",
       " '72452403',\n",
       " '71904521',\n",
       " '68238154',\n",
       " '68293795',\n",
       " '69747537',\n",
       " '72428168',\n",
       " '69776905',\n",
       " '71196096',\n",
       " '69080837',\n",
       " '70104899',\n",
       " '70162126',\n",
       " '71902193',\n",
       " '72697363',\n",
       " '71851279',\n",
       " '72610903',\n",
       " '69249326',\n",
       " '69007802',\n",
       " '69082733',\n",
       " '55409704',\n",
       " '67945529',\n",
       " '71213522',\n",
       " '72485547',\n",
       " '72697019',\n",
       " '69558397',\n",
       " '72243490',\n",
       " '69080968',\n",
       " '72444214',\n",
       " '69080512',\n",
       " '69121415',\n",
       " '69776963',\n",
       " '67441347',\n",
       " '72630989',\n",
       " '69689923',\n",
       " '69250944',\n",
       " '72690626',\n",
       " '72450119',\n",
       " '71820132',\n",
       " '69559906',\n",
       " '71677261',\n",
       " '71292106',\n",
       " '72386497',\n",
       " '69078766',\n",
       " '72427843',\n",
       " '70253617',\n",
       " '72427796',\n",
       " '71125410',\n",
       " '67770798',\n",
       " '69082575',\n",
       " '68849619',\n",
       " '72379491',\n",
       " '71631421']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the cleaned data\n",
    "with open(\"cleaned_descriptions.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    cleaned_descriptions = file.readlines()\n",
    "\n",
    "# Load the vocabulary\n",
    "with open(\"vocab.txt\", \"r\") as file:\n",
    "    vocab = [line.strip().split(\":\")[0] for line in file]\n",
    "\n",
    "# Indicate original data folder\n",
    "original_data_folder = \"data\"\n",
    "\n",
    "# Initiate an empty list to store webindex numbers\n",
    "webindex_numbers = []\n",
    "\n",
    "# Iterate through the original data files and extract webindex\n",
    "for category_folder in os.listdir(original_data_folder):\n",
    "    category_path = os.path.join(original_data_folder, category_folder)\n",
    "    if os.path.isdir(category_path):\n",
    "        for job_file in os.listdir(category_path):\n",
    "            if job_file.startswith(\"Job_\") and job_file.endswith(\".txt\"):\n",
    "                with open(os.path.join(category_path, job_file), \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "                    \n",
    "                    # Extract the webindex from the original data and remove the newline character\n",
    "                    webindex = content.split(\"Webindex: \")[1].split(\"\\n\")[0]\n",
    "                    \n",
    "                    webindex_numbers.append(webindex)\n",
    "\n",
    "webindex_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the CountVectorizer using the vocabulary\n",
    "count_vectorizer = CountVectorizer(vocabulary = vocab)\n",
    "\n",
    "# Fit and transform the preprocessed data to get the BoW representation\n",
    "count_vectors = count_vectorizer.fit_transform(cleaned_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(776, 5168)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings Models:\n",
    "\n",
    "Choose FastText as the embedding language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=2741, vector_size=200, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# Train the model using words from cleaned descriptions\n",
    "\n",
    "# Set vector size and corpus file\n",
    "file = 'cleaned_descriptions.txt'\n",
    "model = FastText(vector_size = 200)\n",
    "model.build_vocab(corpus_file = file)\n",
    "\n",
    "# Train the model\n",
    "model.train(corpus_file = file, \n",
    "                     epochs = model.epochs, \n",
    "                     total_examples = model.corpus_count, \n",
    "                     total_words = model.corpus_total_words)\n",
    "\n",
    "# See model overview\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained FastText model to a file\n",
    "model_path = 'ft_model.bin'\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained FastText model\n",
    "model_path = 'ft_model.bin'\n",
    "ft_model = FastText.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate unweighted word embeddings and also handle missing words\n",
    "def gen_unweighted(data, model):\n",
    "    unweighted_word_embeddings = []\n",
    "    \n",
    "    for text in data:\n",
    "        \n",
    "        # Split text into tokens\n",
    "        tokens = text.split()\n",
    "        \n",
    "        # Initiate an empty list to store unweighted embeddings\n",
    "        unweighted_embeddings = []\n",
    "\n",
    "        for token in tokens:\n",
    "            \n",
    "            # If the token is in the model's vocabulary, get its embedding\n",
    "            if token in model.wv.key_to_index:\n",
    "                word_vec = model.wv.get_vector(token)\n",
    "                unweighted_embeddings.append(word_vec)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # Handle missing words by replacing with a zero vector\n",
    "                unweighted_embeddings.append(np.zeros(model.vector_size))\n",
    "\n",
    "        # Calculate the mean of unweighted word embeddings for this text\n",
    "        if unweighted_embeddings:\n",
    "            unweighted_mean_embedding = np.mean(unweighted_embeddings, axis = 0)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Again, if no valid tokens, use a zero vector\n",
    "            unweighted_mean_embedding = np.zeros(model.vector_size)\n",
    "\n",
    "        unweighted_word_embeddings.append(unweighted_mean_embedding)\n",
    "\n",
    "    return unweighted_word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.16085886e-01,  1.31698322e-01,  1.42177568e-01, -1.93397799e-01,\n",
       "        4.06616063e-01, -3.01417670e-02, -5.77941797e-02, -2.02977796e-01,\n",
       "        8.99173869e-02, -1.21408529e-01, -1.16406157e-01, -1.14120013e-02,\n",
       "       -3.41974583e-01, -1.77978319e-01, -3.52121552e-01, -8.55907139e-02,\n",
       "        2.80723544e-02, -1.41619433e-01, -8.32215895e-02, -1.32550846e-01,\n",
       "       -1.02289083e-01,  4.88783133e-02, -2.28657142e-02, -1.41454551e-02,\n",
       "       -2.07850342e-01,  1.74182147e-01,  1.21845290e-02,  2.43405515e-01,\n",
       "        4.44324232e-01,  1.94856189e-01,  3.75705325e-02, -5.84976507e-02,\n",
       "        2.53603580e-01,  2.95438035e-02,  1.20897621e-02,  3.09662810e-01,\n",
       "       -1.33661314e-01, -1.63215300e-01, -2.11922777e-01,  3.95339357e-01,\n",
       "        7.55148386e-02,  5.49730560e-02,  1.23371840e-01,  5.37294854e-01,\n",
       "        4.77035165e-02, -2.94763239e-01,  5.74185361e-03, -4.45274959e-02,\n",
       "        9.38234671e-02, -3.07710857e-02, -4.38175227e-01,  3.58605801e-04,\n",
       "        1.90199921e-01,  2.51897855e-01,  1.64517723e-01,  9.03535331e-02,\n",
       "       -7.50951436e-02, -1.07949418e-02, -1.32729737e-01,  8.13420353e-02,\n",
       "       -1.41403920e-01,  2.09565262e-01, -3.16000881e-01, -2.41740692e-01,\n",
       "        5.80209119e-02,  6.84747554e-02,  4.16077297e-02, -1.81933606e-01,\n",
       "        1.03367457e-01, -3.95365939e-01,  2.36444818e-01,  1.60376286e-01,\n",
       "        3.97056067e-02,  1.40226739e-01,  2.25352762e-01, -4.35322015e-01,\n",
       "        1.38065619e-02, -1.31433815e-01, -1.26646615e-01, -1.67429527e-01,\n",
       "       -3.12467420e-01, -4.44900558e-02, -2.72860280e-01, -5.01402044e-01,\n",
       "       -1.83002157e-01, -1.20512071e-01,  1.48755613e-01,  3.58178078e-01,\n",
       "        3.50858205e-02,  1.58210100e-01, -1.93705679e-01,  4.86839290e-01,\n",
       "        4.54418340e-01, -3.05592090e-01,  1.95724849e-01,  1.16992845e-01,\n",
       "        1.23155614e-01, -3.69689200e-01, -2.60923914e-01,  3.28946287e-01,\n",
       "       -1.95846957e-01, -1.43360343e-01, -1.92373435e-01, -1.28210180e-01,\n",
       "       -2.02693396e-01, -2.41140275e-01,  1.71919626e-01,  4.68351674e-02,\n",
       "       -1.37834786e-01, -2.98164683e-01, -2.77368741e-01,  9.58918586e-02,\n",
       "        4.64884626e-01, -1.06331852e-01,  2.70460326e-01,  1.61108278e-02,\n",
       "        2.18724873e-01, -1.56560422e-01, -1.70473033e-01,  1.90917439e-01,\n",
       "       -1.59706630e-01,  4.59678577e-02,  3.99131300e-02, -4.43091634e-02,\n",
       "       -7.99220987e-02, -4.48997515e-02, -1.22275346e-01,  3.07677264e-01,\n",
       "       -3.85908838e-01,  1.00891101e-01, -3.31719263e-02, -6.81865785e-02,\n",
       "        9.04567701e-02,  1.63078483e-01, -2.97639945e-01,  8.33599603e-02,\n",
       "        3.21384204e-01, -8.72859270e-02,  2.65160657e-01, -2.81929166e-02,\n",
       "       -2.33026958e-01,  1.42474842e-01, -1.90263371e-01, -1.94574151e-01,\n",
       "        6.22362423e-02,  1.64474160e-01,  2.09820037e-01,  2.80864962e-01,\n",
       "       -8.01415829e-02, -2.19882935e-01,  1.52037733e-01, -1.24019943e-01,\n",
       "       -3.39659423e-01, -3.47791133e-01,  2.79294536e-01, -5.81544396e-02,\n",
       "       -1.72847700e-01, -4.11944811e-01, -1.50532427e-02, -2.69160652e-01,\n",
       "        2.18492231e-01, -7.44527666e-02,  7.46450384e-03, -1.51268663e-01,\n",
       "        2.68115146e-01,  2.96619325e-01, -2.60437086e-02,  3.70765722e-03,\n",
       "       -7.05102836e-02,  6.71035817e-03,  7.56115348e-02,  2.41348194e-01,\n",
       "        1.29949300e-01,  7.95391198e-02,  2.73567099e-01, -3.18596324e-02,\n",
       "       -8.85723009e-02,  3.48838061e-01, -3.92613073e-02, -2.01972512e-01,\n",
       "        2.13601962e-01,  5.45171865e-02, -1.00807492e-01, -1.08946870e-01,\n",
       "       -2.17162350e-01,  1.25307245e-01, -8.31796324e-02, -1.72156112e-03,\n",
       "       -9.50172113e-02, -2.79494430e-01,  6.35796245e-02, -3.28268330e-02,\n",
       "        3.05448852e-01, -3.20212095e-01, -6.44295006e-04,  2.39180267e-02,\n",
       "       -2.77524121e-01, -1.51284839e-01,  3.01332445e-02,  3.05884208e-01])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate unweighted word embeddings with handling missing words for the preprocessed data\n",
    "unweighted_descriptions = gen_unweighted(cleaned_descriptions, ft_model)\n",
    "\n",
    "# Print example\n",
    "unweighted_descriptions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate TF-IDF weighted word embeddings\n",
    "def gen_tfidf_weighted(data, model):\n",
    "    \n",
    "    # Create a TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(vocabulary = model.wv.index_to_key)\n",
    "    tfidf_vectors = tfidf_vectorizer.fit_transform(data)\n",
    "    \n",
    "    tfidf_weighted_word_embeddings = []\n",
    "    \n",
    "    for tfidf_vector in tfidf_vectors:\n",
    "        \n",
    "        # Convert the TF-IDF vector to an array\n",
    "        tfidf_array = tfidf_vector.toarray()[0]\n",
    "        \n",
    "        # Calculate the weighted mean embedding using TF-IDF weights\n",
    "        weighted_embedding = np.sum(\n",
    "            tfidf_array[i] * model.wv.get_vector(token) if token in model.wv.key_to_index else np.zeros(model.vector_size)\n",
    "            for i, token in enumerate(tfidf_vectorizer.get_feature_names_out())\n",
    "        )\n",
    "        \n",
    "        tfidf_weighted_word_embeddings.append(weighted_embedding)\n",
    "    \n",
    "    return tfidf_weighted_word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4p/s8hxpn3d5d38mnm356plktmc0000gn/T/ipykernel_41847/2108361969.py:16: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  weighted_embedding = np.sum(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.8355283e+00,  7.6412028e-01,  8.1532222e-01, -1.1226172e+00,\n",
       "        2.3467407e+00, -1.7220733e-01, -3.3090088e-01, -1.1794643e+00,\n",
       "        5.1841837e-01, -7.0068049e-01, -6.7607397e-01, -6.1454147e-02,\n",
       "       -1.9713411e+00, -1.0359097e+00, -2.0333958e+00, -4.9681216e-01,\n",
       "        1.5987560e-01, -8.1698257e-01, -4.8479760e-01, -7.6957029e-01,\n",
       "       -5.9300935e-01,  2.7846721e-01, -1.3200891e-01, -8.2167700e-02,\n",
       "       -1.2008241e+00,  1.0093498e+00,  7.6582752e-02,  1.4041541e+00,\n",
       "        2.5691733e+00,  1.1274645e+00,  2.1495555e-01, -3.3625132e-01,\n",
       "        1.4682111e+00,  1.7501213e-01,  7.0661485e-02,  1.7895256e+00,\n",
       "       -7.8046918e-01, -9.4779813e-01, -1.2235588e+00,  2.2896054e+00,\n",
       "        4.3282560e-01,  3.1813496e-01,  7.1764320e-01,  3.1026547e+00,\n",
       "        2.7679476e-01, -1.7087544e+00,  3.5239469e-02, -2.5133288e-01,\n",
       "        5.3907430e-01, -1.7997757e-01, -2.5379241e+00,  6.9880309e-03,\n",
       "        1.1036607e+00,  1.4613020e+00,  9.5127404e-01,  5.2258557e-01,\n",
       "       -4.3282047e-01, -6.0287490e-02, -7.6633477e-01,  4.6954009e-01,\n",
       "       -8.1300771e-01,  1.2064816e+00, -1.8219936e+00, -1.3911222e+00,\n",
       "        3.3677194e-01,  3.9051113e-01,  2.4019834e-01, -1.0498105e+00,\n",
       "        6.0844934e-01, -2.2861984e+00,  1.3715314e+00,  9.3189985e-01,\n",
       "        2.3008057e-01,  8.0642563e-01,  1.3013620e+00, -2.5195494e+00,\n",
       "        7.6737881e-02, -7.6564330e-01, -7.2755861e-01, -9.7525084e-01,\n",
       "       -1.8100916e+00, -2.5731698e-01, -1.5796661e+00, -2.8977275e+00,\n",
       "       -1.0617518e+00, -6.9084185e-01,  8.5921580e-01,  2.0703492e+00,\n",
       "        2.0811245e-01,  9.1450906e-01, -1.1188761e+00,  2.8125849e+00,\n",
       "        2.6283548e+00, -1.7689328e+00,  1.1345614e+00,  6.8033373e-01,\n",
       "        7.1295696e-01, -2.1397026e+00, -1.5107927e+00,  1.9055225e+00,\n",
       "       -1.1355579e+00, -8.2899874e-01, -1.1122817e+00, -7.4333322e-01,\n",
       "       -1.1761312e+00, -1.3954840e+00,  9.9545491e-01,  2.7632624e-01,\n",
       "       -7.9827166e-01, -1.7235435e+00, -1.6145372e+00,  5.4851300e-01,\n",
       "        2.6844573e+00, -6.1387223e-01,  1.5614587e+00,  9.2487492e-02,\n",
       "        1.2653661e+00, -9.0045565e-01, -9.8634356e-01,  1.1115781e+00,\n",
       "       -9.2709166e-01,  2.6376992e-01,  2.2076049e-01, -2.5927091e-01,\n",
       "       -4.6504918e-01, -2.5903130e-01, -7.0679677e-01,  1.7785938e+00,\n",
       "       -2.2279692e+00,  5.8746952e-01, -1.9490536e-01, -3.9359730e-01,\n",
       "        5.2119821e-01,  9.4326335e-01, -1.7172675e+00,  4.8001796e-01,\n",
       "        1.8662992e+00, -4.9409950e-01,  1.5388995e+00, -1.5737692e-01,\n",
       "       -1.3495507e+00,  8.2487732e-01, -1.0994860e+00, -1.1296737e+00,\n",
       "        3.6301336e-01,  9.4821334e-01,  1.2168784e+00,  1.6236445e+00,\n",
       "       -4.6555403e-01, -1.2750335e+00,  8.8325787e-01, -7.1616030e-01,\n",
       "       -1.9699575e+00, -2.0181112e+00,  1.6180854e+00, -3.3942601e-01,\n",
       "       -1.0002872e+00, -2.3829842e+00, -8.6884759e-02, -1.5630809e+00,\n",
       "        1.2693242e+00, -4.3194315e-01,  4.3684736e-02, -8.7621921e-01,\n",
       "        1.5530851e+00,  1.7233484e+00, -1.4943235e-01,  2.4427265e-02,\n",
       "       -4.0432605e-01,  3.9756369e-02,  4.3937498e-01,  1.3935219e+00,\n",
       "        7.5391710e-01,  4.5721906e-01,  1.5824071e+00, -1.7752355e-01,\n",
       "       -5.1311606e-01,  2.0155475e+00, -2.2211261e-01, -1.1741234e+00,\n",
       "        1.2356530e+00,  3.1037661e-01, -5.8420914e-01, -6.2644380e-01,\n",
       "       -1.2543205e+00,  7.2223336e-01, -4.8092788e-01, -5.0101313e-03,\n",
       "       -5.5070668e-01, -1.6202333e+00,  3.7352604e-01, -1.8926965e-01,\n",
       "        1.7725080e+00, -1.8546760e+00, -1.7250149e-03,  1.4746313e-01,\n",
       "       -1.6062253e+00, -8.7909573e-01,  1.7468414e-01,  1.7678440e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate TF-IDF weighted word embeddings for the preprocessed data\n",
    "tfidf_descriptions = gen_tfidf_weighted(cleaned_descriptions, ft_model)\n",
    "\n",
    "# Print example\n",
    "tfidf_descriptions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving outputs\n",
    "Save the count vector representations into a file named \n",
    "- count_vector.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the count vectors to a file in the required format\n",
    "count_vectors_file = \"count_vectors.txt\"\n",
    "with open(count_vectors_file, 'w', encoding='utf-8') as f:\n",
    "    for webindex, count_vector in zip(webindex_numbers, count_vectors):\n",
    "        # Convert the count vector to a comma-separated string\n",
    "        count_vector_str = ','.join([f\"{i}:{count}\" for i, count in enumerate(count_vector.toarray()[0]) if count > 0])\n",
    "        f.write(f\"#{webindex},{count_vector_str}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Job Advertisement Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models Comparison: Unweighted & TF-IDF Weighted Word Embedding\n",
    "\n",
    "Job categories are derived according to each folder name, namely; Accounting_Finance, Engineering, Healthcare_Nursing, and Sales with index number 0, 1, 2, 3 respectively. This categories will be used in the training and testing using various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load raw files to retrieve job categories from folder names using integers\n",
    "\n",
    "job_data = load_files(r\"data\")  \n",
    "\n",
    "job_categories = job_data.target\n",
    "job_categories = [int(c) for c in job_categories]\n",
    "job_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Q1, We are using 3 different machine learning models to evaluate the accuracy for each feature representation, including:\n",
    "\n",
    "- Logistic Regression Model (Easy to understand and acts as a basic model for comparison)\n",
    "- Random Forest Model (Less likely to overfit and can handle noisy features)\n",
    "- Support Vector Machine Model (Can handle complicated relationships between features and categories.\n",
    "\n",
    "We are using unweighted word embeddings and TF-IDF weighted embeddings as feature representations. Both representations are based on cleaned description from Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Unweighted and TF-IDF Weighted Descriptions for training and testing\n",
    "\n",
    "seed = 15\n",
    "max_iter = 1000\n",
    "\n",
    "# Split the Unweighted embeddings for descriptions into training and testing sets\n",
    "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(unweighted_descriptions, \n",
    "                                                                                 job_categories, \n",
    "                                                                                 list(range(0,len(job_categories))),\n",
    "                                                                                 test_size = 0.2, \n",
    "                                                                                 random_state = seed)\n",
    "\n",
    "# Split the TF-IDF weighted embeddings for descriptions into training and testing sets\n",
    "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(tfidf_descriptions, \n",
    "                                                                                 job_categories, \n",
    "                                                                                 list(range(0,len(job_categories))),\n",
    "                                                                                 test_size = 0.2, \n",
    "                                                                                 random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, choose Logistic Regression Model\n",
    "lr_model = LogisticRegression(random_state = seed, max_iter = max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Unweighted Word Embeddings for Descriptions:\n",
      "\n",
      "Cross-validation scores: [0.42948718 0.41935484 0.43870968 0.44516129 0.49677419]\n",
      "Mean accuracy: 0.4458974358974359\n",
      "Standard deviation: 0.026886636949392504\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score using unweighted word embeddings\n",
    "\n",
    "# Perform 5-fold cross-validation and specify the scoring metric\n",
    "unweighted_description_scores_lr = cross_val_score(lr_model, unweighted_descriptions, \n",
    "                                                   job_categories, cv = 5, scoring = 'accuracy')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Results of Unweighted Word Embeddings for Descriptions:\\n\")\n",
    "print(\"Cross-validation scores:\", unweighted_description_scores_lr)\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores\n",
    "print(\"Mean accuracy:\", unweighted_description_scores_lr.mean())\n",
    "print(\"Standard deviation:\", unweighted_description_scores_lr.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of TF-IDF Weighted Word Embeddings for Descriptions:\n",
      "\n",
      "Cross-validation scores: [0.62820513 0.59354839 0.62580645 0.58709677 0.64516129]\n",
      "Mean accuracy: 0.6159636062861868\n",
      "Standard deviation: 0.02206797345693831\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score using TF-IDF weighted word embeddings\n",
    "\n",
    "# Perform 5-fold cross-validation and specify the scoring metric\n",
    "tfidf_description_scores_lr = cross_val_score(lr_model, tfidf_descriptions, \n",
    "                                              job_categories, cv = 5, scoring = 'accuracy')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Results of TF-IDF Weighted Word Embeddings for Descriptions:\\n\")\n",
    "print(\"Cross-validation scores:\", tfidf_description_scores_lr)\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores\n",
    "print(\"Mean accuracy:\", tfidf_description_scores_lr.mean())\n",
    "print(\"Standard deviation:\", tfidf_description_scores_lr.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, try Random Forest Model\n",
    "rf_model = RandomForestClassifier(n_estimators = 100, random_state = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Unweighted Word Embeddings for Descriptions:\n",
      "\n",
      "Cross-validation scores: [0.51923077 0.52903226 0.50322581 0.56129032 0.54193548]\n",
      "Mean accuracy: 0.5309429280397022\n",
      "Standard deviation: 0.019752796217122354\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score using unweighted word embeddings\n",
    "\n",
    "# Perform 5-fold cross-validation and specify the scoring metric\n",
    "unweighted_description_scores_rf = cross_val_score(rf_model, unweighted_descriptions,\n",
    "                                                   job_categories, cv = 5, scoring = 'accuracy')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Results of Unweighted Word Embeddings for Descriptions:\\n\")\n",
    "print(\"Cross-validation scores:\", unweighted_description_scores_rf)\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores\n",
    "print(\"Mean accuracy:\", unweighted_description_scores_rf.mean())\n",
    "print(\"Standard deviation:\", unweighted_description_scores_rf.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Unweighted Word Embeddings for Descriptions:\n",
      "\n",
      "Cross-validation scores: [0.53846154 0.56774194 0.46451613 0.55483871 0.53548387]\n",
      "Mean accuracy: 0.5322084367245659\n",
      "Standard deviation: 0.03579619270124285\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score using TF-IDF weighted word embeddings\n",
    "\n",
    "# Perform 5-fold cross-validation and specify the scoring metric\n",
    "tfidf_description_scores_rf = cross_val_score(rf_model, tfidf_descriptions, \n",
    "                                              job_categories, cv = 5, scoring = 'accuracy')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Results of Unweighted Word Embeddings for Descriptions:\\n\")\n",
    "print(\"Cross-validation scores:\", tfidf_description_scores_rf)\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores\n",
    "print(\"Mean accuracy:\", tfidf_description_scores_rf.mean())\n",
    "print(\"Standard deviation:\", tfidf_description_scores_rf.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, try Support Vector Machine Model\n",
    "svm_model = SVC(kernel = 'linear', C = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Unweighted Word Embeddings for Descriptions:\n",
      "\n",
      "Cross-validation scores: [0.43589744 0.45806452 0.41935484 0.43870968 0.46451613]\n",
      "Mean accuracy: 0.4433085194375517\n",
      "Standard deviation: 0.016231786535315887\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score using unweighted word embeddings\n",
    "\n",
    "# Perform 5-fold cross-validation and specify the scoring metric\n",
    "unweighted_description_scores_svm = cross_val_score(svm_model, unweighted_descriptions, \n",
    "                                                    job_categories, cv = 5, scoring = 'accuracy')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Results of Unweighted Word Embeddings for Descriptions:\\n\")\n",
    "print(\"Cross-validation scores:\", unweighted_description_scores_svm)\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores\n",
    "print(\"Mean accuracy:\", unweighted_description_scores_svm.mean())\n",
    "print(\"Standard deviation:\", unweighted_description_scores_svm.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Unweighted Word Embeddings for Descriptions:\n",
      "\n",
      "Cross-validation scores: [0.6025641  0.60645161 0.61290323 0.56129032 0.61935484]\n",
      "Mean accuracy: 0.6005128205128205\n",
      "Standard deviation: 0.020427555655148158\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score using TF-IDF weighted word embeddings\n",
    "\n",
    "# Perform 5-fold cross-validation and specify the scoring metric\n",
    "tfidf_description_scores_svm = cross_val_score(svm_model, tfidf_descriptions, \n",
    "                                               job_categories, cv = 5, scoring = 'accuracy')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Results of Unweighted Word Embeddings for Descriptions:\\n\")\n",
    "print(\"Cross-validation scores:\", tfidf_description_scores_svm)\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores\n",
    "print(\"Mean accuracy:\", tfidf_description_scores_svm.mean())\n",
    "print(\"Standard deviation:\", tfidf_description_scores_svm.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy\n",
      "\n",
      "Logistic Regression Model\n",
      "Unweighted:\t\t 0.446\n",
      "TF-IDF Weighted:\t 0.616\n",
      "\n",
      "Random Forest Model\n",
      "Unweighted:\t\t 0.531\n",
      "TF-IDF Weighted:\t 0.532\n",
      "\n",
      "Support Vector Machine Model\n",
      "Unweighted:\t\t 0.443\n",
      "TF-IDF Weighted:\t 0.601\n"
     ]
    }
   ],
   "source": [
    "# Set the rounded decimal points\n",
    "dec = 3\n",
    "\n",
    "# Print comparison\n",
    "print(\"Mean Accuracy\\n\")\n",
    "print(\"Logistic Regression Model\")\n",
    "print(\"Unweighted:\\t\\t\", round(unweighted_description_scores_lr.mean(), dec))\n",
    "print(\"TF-IDF Weighted:\\t\", round(tfidf_description_scores_lr.mean(), dec))\n",
    "print(\"\\nRandom Forest Model\")\n",
    "print(\"Unweighted:\\t\\t\", round(unweighted_description_scores_rf.mean(), dec))\n",
    "print(\"TF-IDF Weighted:\\t\", round(tfidf_description_scores_rf.mean(), dec))\n",
    "print(\"\\nSupport Vector Machine Model\")\n",
    "print(\"Unweighted:\\t\\t\", round(unweighted_description_scores_svm.mean(), dec))\n",
    "print(\"TF-IDF Weighted:\\t\", round(tfidf_description_scores_svm.mean(), dec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Analysis:\n",
    "\n",
    "#### Logistic Regression Model:\n",
    "\n",
    "- The TF-IDF weighted feature representation outperforms the unweighted representation significantly.\n",
    "- This indicates that TF-IDF weighting helps the logistic regression model better capture the distinguishing features among job advertisements.\n",
    "\n",
    "#### Random Forest Model:\n",
    "\n",
    "- Though it is slightly lower, the unweighted and TF-IDF weighted representations have similar mean accuracy scores.\n",
    "- Random Forest is an ensemble model that may not benefit as much from TF-IDF weighting as logistic regression, which relies on linear relationships.\n",
    "\n",
    "#### Support Vector Machine Model\n",
    "\n",
    "- Similar to logistic regression, the TF-IDF weighted feature representation performs better.\n",
    "- SVM, like logistic regression, benefits from TF-IDF weighting as it helps to create better decision boundaries.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "In summary, the choice between unweighted and TF-IDF weighted feature representations depends on the specific machine learning model being used. Logistic regression and support vector machines benefit from TF-IDF weighting, as it helps them capture the importance of words. Random Forest, on the other hand, may not show improvement with TF-IDF weighting, as it can handle non-linear relationships differently, but the difference is not very significant. However, looking at this specific model comparison, it could be concluded that the TF-IDF weighted representation consistently performs better across the majority of models in this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Improvement: Descriptions, Titles, and the Combination of Both\n",
    "\n",
    "To investigate whether including additional information, such as the title of job advertisements, improves the accuracy of our classification models, we begin with extracting the titles from the raw data. But with a different approach, we will not filter out as much as we did with description, as for the fact that titles contain shorter contents. It is very much similar to Task 1 and Task 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Finance / Accounts Asst Bromley to ****k'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract titles from job data\n",
    "\n",
    "titles = []\n",
    "\n",
    "# Define a function to extract the title part from a text\n",
    "def extract_title(text):\n",
    "    start_text = text.find(\"Title: \")\n",
    "    end_text = text.find(\"\\n\")\n",
    "    if start_text != -1:\n",
    "        title = text[start_text + len(\"Title: \"):end_text]\n",
    "        return title\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Iterate through the loaded data and extract titles\n",
    "for text in job_data.data:\n",
    "    title = extract_title(text.decode(\"utf-8\"))\n",
    "    titles.append(title)\n",
    "\n",
    "# See example of the first title    \n",
    "emp = 0\n",
    "titles[emp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract titles from job data\n",
    "\n",
    "webbbb = []\n",
    "\n",
    "# Define a function to extract the title part from a text\n",
    "def extract_webbb(text):\n",
    "    start_text = text.find(\"Webindex: \")\n",
    "    end_text = text.find(\"\\n\")\n",
    "    if start_text != -1:\n",
    "        webbb = text[start_text + len(\"Webindex: \"):end_text]\n",
    "        return webbb\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Iterate through the loaded data and extract titles\n",
    "for text in job_data.data:\n",
    "    webbb = extract_webbb(text.decode(\"utf-8\"))\n",
    "    webbbb.append(webbb)\n",
    "\n",
    "# See example of the first title    \n",
    "emp = 1\n",
    "webbbb[emp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to tokenise data\n",
    "\n",
    "def tokenize_data(data_raw):\n",
    "    \"\"\"\n",
    "        This function first convert all words to lowercases, \n",
    "        it then segment the raw review into sentences and tokenize each sentences \n",
    "        and convert the review to a list of tokens.\n",
    "    \"\"\"        \n",
    "    # Convert to lower case\n",
    "    data_lc = data_raw.lower()\n",
    "    \n",
    "    # segament into sentences\n",
    "    sentences = sent_tokenize(data_lc)\n",
    "    \n",
    "    # tokenize each sentence\n",
    "    pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "    tokenizer = RegexpTokenizer(pattern) \n",
    "    token_lists = [tokenizer.tokenize(sen) for sen in sentences]\n",
    "    \n",
    "    # merge them into a list of tokens\n",
    "    data_tokenised = list(chain.from_iterable(token_lists))\n",
    "    return data_tokenised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to print the current stats\n",
    "\n",
    "def stats_print(data_tk):\n",
    "    words = list(chain.from_iterable(data_tk))\n",
    "    vocab = set(words)\n",
    "    lexical_diversity = len(vocab)/len(words)\n",
    "    print(\"Vocabulary size: \",len(vocab))\n",
    "    print(\"Total number of tokens: \", len(words))\n",
    "    print(\"Lexical diversity: \", lexical_diversity)\n",
    "    print(\"Total number of reviews:\", len(data_tk))\n",
    "    lens = [len(article) for article in data_tk]\n",
    "    print(\"Average review length:\", np.mean(lens))\n",
    "    print(\"Maximun review length:\", np.max(lens))\n",
    "    print(\"Minimun review length:\", np.min(lens))\n",
    "    print(\"Standard deviation of review length:\", np.std(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " Finance / Accounts Asst Bromley to ****k \n",
      "\n",
      "Tokenized Data:\n",
      " ['finance', 'accounts', 'asst', 'bromley', 'to', 'k']\n"
     ]
    }
   ],
   "source": [
    "# Tokenise the data and compare the result with the original data\n",
    "\n",
    "title_tk = [tokenize_data(d) for d in titles]\n",
    "\n",
    "print(\"Original Data:\\n\", titles[emp],'\\n')\n",
    "print(\"Tokenized Data:\\n\", title_tk[emp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  1003\n",
      "Total number of tokens:  3157\n",
      "Lexical diversity:  0.3177066835603421\n",
      "Total number of reviews: 776\n",
      "Average review length: 4.068298969072165\n",
      "Maximun review length: 13\n",
      "Minimun review length: 1\n",
      "Standard deviation of review length: 1.8386529115562282\n"
     ]
    }
   ],
   "source": [
    "# Check overall stats\n",
    "\n",
    "stats_print(title_tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Data with at least 2 characters:\n",
      " ['finance', 'accounts', 'asst', 'bromley', 'to']\n"
     ]
    }
   ],
   "source": [
    "# Filter out single character tokens\n",
    "title_tk = [[w for w in words if len(w) >=2] for words in title_tk]\n",
    "\n",
    "print(\"Tokenized Data with at least 2 characters:\\n\", title_tk[emp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Data excluding stop words:\n",
      " ['finance', 'accounts', 'asst', 'bromley']\n"
     ]
    }
   ],
   "source": [
    "# Import stop words from the required file\n",
    "\n",
    "stopwords_file = \"stopwords_en.txt\"\n",
    "with open(stopwords_file, 'r') as f:\n",
    "    stop_words = set(f.read().split())\n",
    "    \n",
    "# Filter out stop words\n",
    "\n",
    "title_tk = [[w for w in words if w not in stop_words] for words in title_tk]\n",
    "\n",
    "print(\"Tokenized Data excluding stop words:\\n\", title_tk[emp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  954\n",
      "Total number of tokens:  2963\n",
      "Lexical diversity:  0.32197097536280794\n",
      "Total number of reviews: 776\n",
      "Average review length: 3.818298969072165\n",
      "Maximun review length: 10\n",
      "Minimun review length: 1\n",
      "Standard deviation of review length: 1.5653217426587334\n"
     ]
    }
   ],
   "source": [
    "# Check stats after cleansing\n",
    "\n",
    "stats_print(title_tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine tokens and save output as a text file\n",
    "\n",
    "combined_data = [\" \".join(tokens) for tokens in title_tk]\n",
    "output_file = \"cleaned_titles.txt\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for title in combined_data:\n",
    "        f.write(title + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned titles\n",
    "with open(\"cleaned_titles.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    cleaned_titles = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create feature representations just like what we did for descriptions and then test it with the model. The difference now is that we are not comparing the performance of different models. our main focus is to assess whether the inclusion of additional information, such as the job title, enhances the accuracy of our classification model. We use only the Logistic Regression Model for this comparison by having different combinations of feature representation as followings:\n",
    "\n",
    "- Using Only Descriptions\n",
    "- Using Only Titles\n",
    "- Using Both Description and Titles\n",
    "\n",
    "We have done the descriptions in Q1 already, now we have to test for titles and the combination of both. The feature representations for titles will first be created. And later, we will combine it with feature representations for descriptions we have done earlier to come up with the feature representations for the combination of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.21515532,  0.05511866,  0.03388772, -0.14548835,  0.128492  ,\n",
       "       -0.02563727, -0.00583292, -0.1600173 ,  0.05758339, -0.04659123,\n",
       "       -0.06965681,  0.00988455, -0.14751576, -0.15219685, -0.15865776,\n",
       "       -0.08833994, -0.05861265, -0.05649972, -0.07056785, -0.09055908,\n",
       "       -0.07902582, -0.0107549 , -0.01048948, -0.04635653, -0.05785845,\n",
       "        0.08659559,  0.01476522,  0.16696759,  0.24409226,  0.05835579,\n",
       "        0.01647919, -0.02232521,  0.15415183,  0.03250293, -0.00145524,\n",
       "        0.18080746, -0.12884754, -0.05294625, -0.08375809,  0.29147463,\n",
       "        0.03595039,  0.0235871 ,  0.12984306,  0.25126754,  0.05035678,\n",
       "       -0.25617716,  0.00262706,  0.01418623,  0.0443195 , -0.0508069 ,\n",
       "       -0.20478545,  0.0030441 ,  0.11874967,  0.18297692,  0.13667066,\n",
       "        0.06711508, -0.02365233,  0.04280203, -0.07624581,  0.06980649,\n",
       "       -0.07172519,  0.04018674, -0.14695258, -0.05339774,  0.05073516,\n",
       "       -0.0473206 ,  0.00140917, -0.10365963,  0.20051552, -0.24308888,\n",
       "        0.15949427,  0.09608404,  0.00797843,  0.03735632,  0.06706896,\n",
       "       -0.21157268, -0.01715946, -0.14713307, -0.06657175, -0.11088452,\n",
       "       -0.25298679, -0.02153619, -0.22665881, -0.25588553, -0.13081135,\n",
       "       -0.02509069,  0.02975303,  0.14729995,  0.11569463,  0.08614499,\n",
       "       -0.09410948,  0.2327351 ,  0.22624101, -0.17917781,  0.14768449,\n",
       "        0.0483127 ,  0.0142407 , -0.22169705, -0.1518369 ,  0.22082777,\n",
       "       -0.15770672, -0.09723531, -0.15210573, -0.10708131, -0.09349357,\n",
       "       -0.10273702,  0.07128448,  0.0574594 , -0.06205804, -0.21108119,\n",
       "       -0.25141707, -0.00201715,  0.21236897, -0.00835639,  0.11861563,\n",
       "       -0.00079047,  0.1450344 , -0.06613565, -0.12596575,  0.12752098,\n",
       "       -0.10831353,  0.00923623, -0.07937888, -0.08152843, -0.08269891,\n",
       "       -0.06682582, -0.09297561,  0.16154542, -0.15622506,  0.07937534,\n",
       "       -0.04100745, -0.0637792 ,  0.06509604,  0.10481327, -0.07460633,\n",
       "        0.06831384,  0.20779986, -0.02765253,  0.16500398,  0.07814249,\n",
       "       -0.19246802,  0.13770563, -0.07842676, -0.17046789,  0.03692297,\n",
       "        0.08757134,  0.16704733,  0.22939792, -0.09010407, -0.14145367,\n",
       "        0.07274896, -0.08183654, -0.19259711, -0.28420402,  0.15257011,\n",
       "       -0.01729393, -0.11877616, -0.26130225, -0.00283732, -0.17656156,\n",
       "        0.18222293, -0.02793445,  0.01822251, -0.09254242,  0.17226031,\n",
       "        0.22543225, -0.02543539,  0.03748768, -0.03966893, -0.01017068,\n",
       "        0.01151126,  0.16671202,  0.07827883,  0.03280677,  0.12370436,\n",
       "        0.06875241, -0.06827477,  0.18914946, -0.02438478, -0.19466669,\n",
       "        0.09320422,  0.02420759, -0.06127074, -0.06849303, -0.09790848,\n",
       "        0.04876997, -0.10066559, -0.02265501, -0.04498415, -0.18747811,\n",
       "        0.05181334, -0.034023  ,  0.21750012, -0.20506981,  0.0022922 ,\n",
       "        0.07857274, -0.14813996, -0.12431003,  0.00387931,  0.11906519])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate unweighted word embeddings with handling missing words for the preprocessed data\n",
    "unweighted_titles = gen_unweighted(cleaned_titles, ft_model)\n",
    "\n",
    "# Print example\n",
    "unweighted_titles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4p/s8hxpn3d5d38mnm356plktmc0000gn/T/ipykernel_41847/2108361969.py:16: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  weighted_embedding = np.sum(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.6117287 ,  0.15545551,  0.0920928 , -0.41432494,  0.35682794,\n",
       "       -0.07238081, -0.01507537, -0.4555808 ,  0.16283438, -0.13068989,\n",
       "       -0.1968213 ,  0.02939172, -0.41377914, -0.4333533 , -0.44627827,\n",
       "       -0.25225547, -0.170574  , -0.15794107, -0.2012933 , -0.2579033 ,\n",
       "       -0.2246464 , -0.03302472, -0.02899878, -0.13258171, -0.15978324,\n",
       "        0.24461642,  0.04358194,  0.47356588,  0.69021434,  0.16246039,\n",
       "        0.04604499, -0.06245265,  0.436869  ,  0.09366398, -0.00391887,\n",
       "        0.51171   , -0.36790434, -0.14879237, -0.23486975,  0.8283162 ,\n",
       "        0.10063478,  0.06558502,  0.37092698,  0.70719737,  0.14355606,\n",
       "       -0.7295722 ,  0.0076462 ,  0.04319708,  0.1244489 , -0.14593127,\n",
       "       -0.5773063 ,  0.00892595,  0.33709264,  0.5203637 ,  0.3889023 ,\n",
       "        0.19064774, -0.06569603,  0.12407997, -0.21603751,  0.19808936,\n",
       "       -0.20198476,  0.10916024, -0.41299608, -0.14580412,  0.14440708,\n",
       "       -0.13885088,  0.00254008, -0.29248828,  0.5763472 , -0.6883478 ,\n",
       "        0.45243907,  0.2722837 ,  0.02156677,  0.10196952,  0.18611552,\n",
       "       -0.5968243 , -0.04996438, -0.42059165, -0.18809542, -0.3148433 ,\n",
       "       -0.719687  , -0.0605828 , -0.64474535, -0.72198784, -0.372013  ,\n",
       "       -0.06791409,  0.08079636,  0.4134025 ,  0.33283687,  0.24301457,\n",
       "       -0.2654753 ,  0.6554798 ,  0.63813174, -0.5068573 ,  0.42010438,\n",
       "        0.13618138,  0.03717608, -0.62803733, -0.43031555,  0.62623566,\n",
       "       -0.44872445, -0.27553308, -0.4318381 , -0.30497563, -0.2640568 ,\n",
       "       -0.28897148,  0.20014358,  0.16463009, -0.17462362, -0.59885573,\n",
       "       -0.7167307 , -0.00972706,  0.5975305 , -0.02065503,  0.33301193,\n",
       "       -0.00268223,  0.41127583, -0.18600991, -0.3577345 ,  0.3625565 ,\n",
       "       -0.30751753,  0.02520398, -0.2310274 , -0.2337763 , -0.23672181,\n",
       "       -0.19154787, -0.26390368,  0.4559198 , -0.43707305,  0.22670361,\n",
       "       -0.11672712, -0.18234617,  0.1842956 ,  0.296858  , -0.20559204,\n",
       "        0.1942823 ,  0.59007215, -0.07666358,  0.46792775,  0.22633226,\n",
       "       -0.54784995,  0.39225292, -0.21998769, -0.48561478,  0.10503162,\n",
       "        0.24694757,  0.4753533 ,  0.6523559 , -0.2570448 , -0.4010927 ,\n",
       "        0.20557013, -0.23212935, -0.54518586, -0.80975133,  0.43108135,\n",
       "       -0.04814684, -0.33710483, -0.7405741 , -0.0073808 , -0.5016511 ,\n",
       "        0.5188501 , -0.07842329,  0.05225199, -0.2622818 ,  0.4889465 ,\n",
       "        0.64158   , -0.07250424,  0.10782618, -0.11145812, -0.0289847 ,\n",
       "        0.03159785,  0.47314858,  0.22221226,  0.09143583,  0.34830648,\n",
       "        0.20014308, -0.19380355,  0.5340551 , -0.06823166, -0.5558998 ,\n",
       "        0.26186687,  0.06733113, -0.17388624, -0.19430736, -0.27488118,\n",
       "        0.1356543 , -0.28726968, -0.06457524, -0.12711143, -0.53224134,\n",
       "        0.14814697, -0.09659389,  0.61823684, -0.581398  ,  0.00690255,\n",
       "        0.22669534, -0.41774958, -0.3536361 ,  0.01017965,  0.33336228],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate TF-IDF weighted word embeddings for the preprocessed data\n",
    "tfidf_titles = gen_tfidf_weighted(cleaned_titles, ft_model)\n",
    "\n",
    "# Print example\n",
    "tfidf_titles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Unweighted and TF-IDF Weighted Titles for training and testing\n",
    "\n",
    "seed = 15\n",
    "max_iter = 1000\n",
    "\n",
    "# Split the unweighted embeddings for titles into training and testing sets\n",
    "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(unweighted_titles, \n",
    "                                                                                 job_categories, \n",
    "                                                                                 list(range(0,len(job_categories))),\n",
    "                                                                                 test_size = 0.2, \n",
    "                                                                                 random_state = seed)\n",
    "\n",
    "# Split the TF-IDF weighted embeddings for titles into training and testing sets\n",
    "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(tfidf_titles, \n",
    "                                                                                 job_categories, \n",
    "                                                                                 list(range(0,len(job_categories))),\n",
    "                                                                                 test_size = 0.2, \n",
    "                                                                                 random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Unweighted Word Embeddings for Titles:\n",
      "\n",
      "Cross-validation scores: [0.43589744 0.43870968 0.46451613 0.53548387 0.53548387]\n",
      "Mean accuracy: 0.48201819685690656\n",
      "Standard deviation: 0.04477997642193246\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score using unweighted word embeddings\n",
    "\n",
    "# Perform 5-fold cross-validation and specify the scoring metric\n",
    "unweighted_title_scores_lr = cross_val_score(lr_model, unweighted_titles, \n",
    "                                                   job_categories, cv = 5, scoring = 'accuracy')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Results of Unweighted Word Embeddings for Titles:\\n\")\n",
    "print(\"Cross-validation scores:\", unweighted_title_scores_lr)\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores\n",
    "print(\"Mean accuracy:\", unweighted_title_scores_lr.mean())\n",
    "print(\"Standard deviation:\", unweighted_title_scores_lr.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of TF-IDF Weighted Word Embeddings for Titles:\n",
      "\n",
      "Cross-validation scores: [0.51923077 0.47741935 0.52903226 0.58064516 0.50967742]\n",
      "Mean accuracy: 0.5232009925558313\n",
      "Standard deviation: 0.033551286820224145\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score using TF-IDF weighted word embeddings\n",
    "\n",
    "# Perform 5-fold cross-validation and specify the scoring metric\n",
    "tfidf_title_scores_lr = cross_val_score(lr_model, tfidf_titles, \n",
    "                                              job_categories, cv = 5, scoring = 'accuracy')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Results of TF-IDF Weighted Word Embeddings for Titles:\\n\")\n",
    "print(\"Cross-validation scores:\", tfidf_title_scores_lr)\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores\n",
    "print(\"Mean accuracy:\", tfidf_title_scores_lr.mean())\n",
    "print(\"Standard deviation:\", tfidf_title_scores_lr.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we are combining the two representations then test them for the last set of comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate unweighted word embeddings for titles and descriptions horizontally\n",
    "unweighted_combined = np.hstack((unweighted_descriptions, unweighted_titles))\n",
    "tfidf_combined = np.hstack((tfidf_descriptions, tfidf_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Unweighted and TF-IDF Weighted Descriptions and Titles for training and testing\n",
    "\n",
    "seed = 15\n",
    "max_iter = 1000\n",
    "\n",
    "# Split the unweighted embeddings for both descriptions and titles into training and testing sets\n",
    "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(unweighted_combined, \n",
    "                                                                                 job_categories, \n",
    "                                                                                 list(range(0,len(job_categories))),\n",
    "                                                                                 test_size = 0.2, \n",
    "                                                                                 random_state = seed)\n",
    "\n",
    "# Split the TF-IDF weighted embeddings for both descriptions and titles into training and testing sets\n",
    "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(tfidf_combined, \n",
    "                                                                                 job_categories, \n",
    "                                                                                 list(range(0,len(job_categories))),\n",
    "                                                                                 test_size = 0.2, \n",
    "                                                                                 random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Unweighted Word Embeddings for Descriptions and Titles:\n",
      "\n",
      "Cross-validation scores: [0.48717949 0.50967742 0.48387097 0.5483871  0.52903226]\n",
      "Mean accuracy: 0.5116294458229943\n",
      "Standard deviation: 0.024601328288293988\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score using unweighted word embeddings\n",
    "\n",
    "# Perform 5-fold cross-validation and specify the scoring metric\n",
    "unweighted_combined_scores_lr = cross_val_score(lr_model, unweighted_combined, \n",
    "                                                job_categories, cv = 5, scoring = 'accuracy')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Results of Unweighted Word Embeddings for Descriptions and Titles:\\n\")\n",
    "print(\"Cross-validation scores:\", unweighted_combined_scores_lr)\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores\n",
    "print(\"Mean accuracy:\", unweighted_combined_scores_lr.mean())\n",
    "print(\"Standard deviation:\", unweighted_combined_scores_lr.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of TF-IDF Weighted Word Embeddings for Descriptions and Titles:\n",
      "\n",
      "Cross-validation scores: [0.6474359  0.58064516 0.62580645 0.63870968 0.65806452]\n",
      "Mean accuracy: 0.6301323407775021\n",
      "Standard deviation: 0.026910534916375437\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score using TF-IDF weighted word embeddings\n",
    "\n",
    "# Perform 5-fold cross-validation and specify the scoring metric\n",
    "tfidf_combined_scores_lr = cross_val_score(lr_model, tfidf_combined, \n",
    "                                           job_categories, cv = 5, scoring = 'accuracy')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Results of TF-IDF Weighted Word Embeddings for Descriptions and Titles:\\n\")\n",
    "print(\"Cross-validation scores:\", tfidf_combined_scores_lr)\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the scores\n",
    "print(\"Mean accuracy:\", tfidf_combined_scores_lr.mean())\n",
    "print(\"Standard deviation:\", tfidf_combined_scores_lr.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy\n",
      "\n",
      "Unweighted\n",
      "Only Descriptions:\t 0.446\n",
      "Only Titles:\t\t 0.482\n",
      "Combination:\t\t 0.512\n",
      "\n",
      "TF-IDF Weighted\n",
      "Only Descriptions:\t 0.616\n",
      "Only Titles:\t\t 0.523\n",
      "Combination:\t\t 0.63\n"
     ]
    }
   ],
   "source": [
    "# Set the rounded decimal points\n",
    "dec = 3\n",
    "\n",
    "# Print comparison\n",
    "print(\"Mean Accuracy\\n\")\n",
    "print(\"Unweighted\")\n",
    "print(\"Only Descriptions:\\t\", round(unweighted_description_scores_lr.mean(), dec))\n",
    "print(\"Only Titles:\\t\\t\", round(unweighted_title_scores_lr.mean(), dec))\n",
    "print(\"Combination:\\t\\t\", round(unweighted_combined_scores_lr.mean(), dec))\n",
    "print(\"\\nTF-IDF Weighted\")\n",
    "print(\"Only Descriptions:\\t\", round(tfidf_description_scores_lr.mean(), dec))\n",
    "print(\"Only Titles:\\t\\t\", round(tfidf_title_scores_lr.mean(), dec))\n",
    "print(\"Combination:\\t\\t\", round(tfidf_combined_scores_lr.mean(), dec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Analysis:\n",
    "\n",
    "#### Unweighted Representations:\n",
    "\n",
    "- Looking at the mean accuracy achieved, it suggests that using only job descriptions alone for classification leads to relatively low accuracy.\n",
    "- When using only job titles, the mean accuracy increases. This indicates that titles contribute additional information and improve classification performance compared to descriptions alone.\n",
    "- By combining both job descriptions and titles, the mean accuracy further improves. This suggests that leveraging both sources of information results in better classification accuracy than using either one individually.\n",
    "\n",
    "#### TF-IDF Weighted Representations:\n",
    "\n",
    "- When considering only descriptions, the mean accuracy significantly improves compared to the unweighted representation. TF-IDF weighting seems to enhance the model's ability to classify job advertisements based on descriptions.\n",
    "- When using only job titles, the mean accuracy is also getting better compared to unweighted representations. This indicates that job titles provide valuable information, and TF-IDF weighting seems to have as substantial an impact here as it does with descriptions.\n",
    "- Combining both job descriptions and titles while using TF-IDF weighting results in the best mean accuracy score. This demonstrates that the combination of both textual sources, when weighted with TF-IDF, yields a significant impact on classification accuracy.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "TF-IDF weighting generally improves classification accuracy compared to unweighted representations for both descriptions and titles. It is also important to point out that combining job descriptions and titles consistently leads to improved accuracy across both unweighted and TF-IDF weighted scenarios. TF-IDF weighted descriptions alone also achieve  quite high accuracy score, emphasising their significance in the classification process. Still, in summary, the results indicate that including both job descriptions and titles, especially when TF-IDF Weighted is applied, leads to the best classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "These tasks helped preprocess and represent job advertisement data effectively and evaluate different models for categorising job advertisements. The choice of models, feature representations, and data combination strategies impacted classification accuracy. To improve job advertisement classification further, we can consider exploring and analysing the data, fine-tuning model settings, incorporating additional features, experimenting with various machine learning models, and utilising more advanced natural language processing tools. Though I'm not an expert, through this assignment, I'm getting a better picture of Data Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Couple of notes for all code blocks in this notebook\n",
    "- please provide proper comment on your code\n",
    "- Please re-start and run all cells to make sure codes are runable and include your output in the submission.   \n",
    "<span style=\"color: red\"> This markdown block can be removed once the task is completed. </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
